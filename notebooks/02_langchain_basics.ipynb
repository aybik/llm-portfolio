{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Basics: Building LLM Applications\n",
        "\n",
        "LangChain is a framework for developing applications powered by language models. It provides tools to:\n",
        "- Chain multiple LLM calls together\n",
        "- Manage prompts and templates\n",
        "- Handle conversation memory\n",
        "- Integrate external tools and data sources\n",
        "\n",
        "## Topics Covered:\n",
        "1. LangChain architecture and components\n",
        "2. LLMs and Chat Models\n",
        "3. Prompt Templates\n",
        "4. Output Parsers\n",
        "5. Chains (Sequential, Transform, Router)\n",
        "6. Memory and Conversation Management\n",
        "7. Callbacks and Debugging\n",
        "8. Building a Simple Chatbot\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "# !pip install -U langchain langchain-openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.prompts'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     PromptTemplate,\n\u001b[32m      8\u001b[39m     ChatPromptTemplate,\n\u001b[32m      9\u001b[39m     HumanMessagePromptTemplate,\n\u001b[32m     10\u001b[39m     SystemMessagePromptTemplate,\n\u001b[32m     11\u001b[39m     FewShotPromptTemplate,\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     LLMChain,\n\u001b[32m     15\u001b[39m     SequentialChain,\n\u001b[32m     16\u001b[39m     SimpleSequentialChain,\n\u001b[32m     17\u001b[39m     TransformChain,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     ConversationBufferMemory,\n\u001b[32m     21\u001b[39m     ConversationBufferWindowMemory,\n\u001b[32m     22\u001b[39m     ConversationSummaryMemory,\n\u001b[32m     23\u001b[39m )\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.prompts'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Prompts live in langchain-core now\n",
        "from langchain_core.prompts import (\n",
        "    PromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    FewShotPromptTemplate,\n",
        ")\n",
        "\n",
        "# Legacy chains + memory moved to langchain-classic\n",
        "from langchain_classic.chains import (\n",
        "    LLMChain,\n",
        "    SequentialChain,\n",
        "    SimpleSequentialChain,\n",
        "    TransformChain,\n",
        ")\n",
        "from langchain_classic.memory import (\n",
        "    ConversationBufferMemory,\n",
        "    ConversationBufferWindowMemory,\n",
        "    ConversationSummaryMemory,\n",
        ")\n",
        "\n",
        "# Output parsers in langchain-core\n",
        "from langchain_core.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "# Callback moved to langchain-community in modern installs\n",
        "from langchain_community.callbacks.manager import get_openai_callback\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "print('âœ… Setup complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/aybikealkan/Library/Caches/pypoetry/virtualenvs/llm-portfolio-fuSmwnhh-py3.12/bin/python\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.prompts'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(sys.executable)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlangchain ok:\u001b[39m\u001b[33m\"\u001b[39m, langchain.__version__)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.prompts'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "print(\"langchain ok:\", langchain.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ChatOpenAI' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm = \u001b[43mChatOpenAI\u001b[49m(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m resp = llm.invoke(\u001b[33m\"\u001b[39m\u001b[33mSay \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpong\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and nothing else.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(resp.content)\n",
            "\u001b[31mNameError\u001b[39m: name 'ChatOpenAI' is not defined"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "resp = llm.invoke(\"Say 'pong' and nothing else.\")\n",
        "print(resp.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LangChain Models: LLMs vs Chat Models\n",
        "\n",
        "LangChain supports two types of models:\n",
        "- **LLM**: Text-in, text-out (legacy completion-style)\n",
        "- **Chat Model**: Messages-in, message-out (modern)\n",
        "\n",
        "We'll use Chat Models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=150,\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What is LangChain in one sentence?\")\n",
        "print(response.content)\n",
        "print('\\nType:', type(response))\n",
        "print('Model:', response.response_metadata.get('model_name', 'N/A'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful Python programming tutor.\"),\n",
        "    HumanMessage(content=\"How do I read a CSV file in Python?\"),\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"You are a {role}.\n",
        "Answer the following question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"role\", \"question\"])\n",
        "\n",
        "formatted_prompt = prompt.format(\n",
        "    role=\"data scientist\",\n",
        "    question=\"What is the difference between correlation and causation?\",\n",
        ")\n",
        "print(formatted_prompt)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response = llm.invoke(formatted_prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chat Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\n",
        "        \"You are a {role} who explains concepts in {style} language.\"\n",
        "    ),\n",
        "    HumanMessagePromptTemplate.from_template(\"Explain {concept} to me.\"),\n",
        "])\n",
        "\n",
        "messages = chat_template.format_messages(\n",
        "    role=\"machine learning expert\",\n",
        "    style=\"simple\",\n",
        "    concept=\"gradient descent\",\n",
        ")\n",
        "\n",
        "for msg in messages:\n",
        "    print(f\"{msg.type}: {msg.content}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-Shot Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
        "]\n",
        "\n",
        "example_template = \"\"\"Input: {input}\n",
        "Output: {output}\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=example_template,\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of each word.\",\n",
        "    suffix=\"Input: {word}\\nOutput:\",\n",
        "    input_variables=[\"word\"],\n",
        ")\n",
        "\n",
        "formatted = few_shot_prompt.format(word=\"big\")\n",
        "print(formatted)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response = llm.invoke(formatted)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Output Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_schemas = [\n",
        "    ResponseSchema(name=\"answer\", description=\"The answer to the question\"),\n",
        "    ResponseSchema(name=\"confidence\", description=\"Confidence level (0-100)\"),\n",
        "    ResponseSchema(name=\"explanation\", description=\"Brief explanation\"),\n",
        "]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the following question.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(question=\"What is the capital of France?\")\n",
        "response = llm.invoke(formatted_prompt)\n",
        "parsed = output_parser.parse(response.content)\n",
        "print(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Chains: Connecting Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Tell me a {adjective} fact about {topic}.\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"topic\"])\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "result = llm_chain.invoke({\"adjective\": \"interesting\", \"topic\": \"machine learning\"})\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sequential Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template_1 = \"\"\"You are a creative business consultant.\n",
        "Generate a company name for a business that makes {product}.\n",
        "Return only the company name, nothing else.\n",
        "\"\"\"\n",
        "prompt_1 = PromptTemplate(template=template_1, input_variables=[\"product\"])\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
        "\n",
        "template_2 = \"\"\"You are a marketing expert.\n",
        "Create a catchy tagline for a company called {company_name}.\n",
        "Return only the tagline, nothing else.\n",
        "\"\"\"\n",
        "prompt_2 = PromptTemplate(template=template_2, input_variables=[\"company_name\"])\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "sequential_chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
        "result = sequential_chain.invoke(\"AI-powered code review tools\")\n",
        "print(result[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transform Chain (fixed output keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_func(inputs: dict) -> dict:\n",
        "    text = inputs[\"text\"]\n",
        "    cleaned = \" \".join(text.split()).lower()\n",
        "    return {\"cleaned_text\": cleaned}\n",
        "\n",
        "transform_chain = TransformChain(\n",
        "    input_variables=[\"text\"],\n",
        "    output_variables=[\"cleaned_text\"],\n",
        "    transform=transform_func,\n",
        ")\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    template=\"Summarize this text in one sentence: {cleaned_text}\",\n",
        "    input_variables=[\"cleaned_text\"],\n",
        ")\n",
        "summarize_chain = LLMChain(llm=llm, prompt=summarize_prompt, output_key=\"summary\")\n",
        "\n",
        "full_chain = SequentialChain(\n",
        "    chains=[transform_chain, summarize_chain],\n",
        "    input_variables=[\"text\"],\n",
        "    output_variables=[\"cleaned_text\", \"summary\"],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "messy_text = \"\"\"   Machine    Learning    is   a   subset   of\n",
        "   ARTIFICIAL     INTELLIGENCE    that    enables    computers\n",
        "   to    LEARN    from    data.   \"\"\"\n",
        "\n",
        "result = full_chain.invoke({\"text\": messy_text})\n",
        "print(\"Cleaned:\", result[\"cleaned_text\"])\n",
        "print(\"Summary:\", result[\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Memory: Maintaining Conversation Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
        "\n",
        "template = \"\"\"The following is a conversation between a human and an AI assistant.\n",
        "The AI is helpful and friendly.\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "AI:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"chat_history\", \"human_input\"])\n",
        "conversation_chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation_chain.invoke({\"human_input\": \"Hi, my name is Alice.\"})[\"text\"])\n",
        "print(conversation_chain.invoke({\"human_input\": \"What's my name?\"})[\"text\"])\n",
        "print(memory.load_memory_variables({}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Callbacks and Cost Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with get_openai_callback() as cb:\n",
        "    _ = llm.invoke(\"What is machine learning?\")\n",
        "    _ = llm.invoke(\"What is deep learning?\")\n",
        "    _ = llm.invoke(\"What is the difference between them?\")\n",
        "\n",
        "print(\"Total Tokens:\", cb.total_tokens)\n",
        "print(\"Prompt Tokens:\", cb.prompt_tokens)\n",
        "print(\"Completion Tokens:\", cb.completion_tokens)\n",
        "print(\"Total Cost (USD):\", cb.total_cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Building a Simple Chatbot (fixed memory keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleChatbot:\n",
        "    \"\"\"A simple chatbot with memory and personality.\"\"\"\n",
        "\n",
        "    def __init__(self, personality: str = \"helpful and friendly\", memory_k: int = 5):\n",
        "        self.llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
        "        self.memory = ConversationBufferWindowMemory(\n",
        "            k=memory_k,\n",
        "            memory_key=\"chat_history\",\n",
        "            input_key=\"human_input\",\n",
        "        )\n",
        "\n",
        "        template = f\"\"\"You are a {personality} AI assistant.\n",
        "\n",
        "{{chat_history}}\n",
        "Human: {{human_input}}\n",
        "AI:\"\"\"\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            template=template,\n",
        "            input_variables=[\"chat_history\", \"human_input\"],\n",
        "        )\n",
        "\n",
        "        self.chain = LLMChain(\n",
        "            llm=self.llm,\n",
        "            prompt=prompt,\n",
        "            memory=self.memory,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        self.total_cost = 0.0\n",
        "\n",
        "    def chat(self, message: str) -> str:\n",
        "        with get_openai_callback() as cb:\n",
        "            response = self.chain.invoke({\"human_input\": message})\n",
        "            self.total_cost += cb.total_cost\n",
        "        return response[\"text\"]\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return self.total_cost\n",
        "\n",
        "    def reset(self):\n",
        "        self.memory.clear()\n",
        "        self.total_cost = 0.0\n",
        "\n",
        "bot = SimpleChatbot(personality=\"witty Python programming expert\")\n",
        "\n",
        "conversation_examples = [\n",
        "    \"Hi! I'm learning Python.\",\n",
        "    \"What's the difference between a list and a tuple?\",\n",
        "    \"Can you show me an example?\",\n",
        "    \"Thanks! What should I learn next?\",\n",
        "]\n",
        "\n",
        "for msg in conversation_examples:\n",
        "    print(\"ðŸ‘¤ You:\", msg)\n",
        "    print(\"ðŸ¤– Bot:\", bot.chat(msg))\n",
        "    print()\n",
        "\n",
        "print(\"Total cost:\", bot.get_cost())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Debugging and Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"Translate '{text}' to {language}.\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\", \"language\"])\n",
        "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
        "\n",
        "result = chain.invoke({\"text\": \"Hello, how are you?\", \"language\": \"Spanish\"})\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Best Practices Summary\n",
        "\n",
        "| Memory Type | Use Case | Pros | Cons |\n",
        "|-------------|----------|------|------|\n",
        "| Buffer | Short conversations | Simple, complete history | Uses many tokens for long chats |\n",
        "| Window Buffer | Medium conversations | Limits token usage | Forgets older context |\n",
        "| Summary | Long conversations | Efficient token usage | Loses some detail |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Practice Exercises\n",
        "\n",
        "- Exercise 1: build a 2-step `SequentialChain` that generates 3 questions then answers them.\n",
        "- Exercise 2: build a sentiment-aware chatbot.\n",
        "- Exercise 3: build a \"code explainer\" chain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Advanced Patterns Preview\n",
        "\n",
        "- RAG (Retrieval-Augmented Generation)\n",
        "- Agents and tool use\n",
        "- Document loaders + vector stores\n",
        "- LangSmith monitoring\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-portfolio-fuSmwnhh-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
