{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fundamentals: Understanding the Basics\n",
    "\n",
    "This notebook covers the fundamental concepts of working with Large Language Models (LLMs).\n",
    "\n",
    "## Topics Covered:\n",
    "1. Setting up API connections\n",
    "2. Basic API calls\n",
    "3. Understanding tokens and tokenization\n",
    "4. Temperature and sampling parameters\n",
    "5. Streaming responses\n",
    "6. Cost estimation\n",
    "7. Testing our custom utils\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "# Add parent directory to path to import our utils\n",
    "sys.path.append('..')\n",
    "from utils.config import Config\n",
    "from utils.text_processing import count_tokens\n",
    "from utils.performance import CostEstimator, timer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Your First LLM API Call\n",
    "\n",
    "Let's start with a simple API call to understand the basic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain what a large language model is in one sentence.\n",
      "\n",
      "Response: A large language model is a type of artificial intelligence system that uses vast amounts of data and complex algorithms to understand and generate human language.\n"
     ]
    }
   ],
   "source": [
    "def simple_completion(prompt: str, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"Make a simple completion request.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test it out\n",
    "prompt = \"Explain what a large language model is in one sentence.\"\n",
    "response = simple_completion(prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "Let's examine what information the API returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESPONSE OBJECT STRUCTURE\n",
      "============================================================\n",
      "\n",
      "Model used: gpt-3.5-turbo-0125\n",
      "ID: chatcmpl-CnQ2XT8zIKlExoqmWIgsD8Vrq9rsG\n",
      "Created: 1765894205\n",
      "\n",
      "Token usage:\n",
      "  - Prompt tokens: 9\n",
      "  - Completion tokens: 9\n",
      "  - Total tokens: 18\n",
      "\n",
      "Finish reason: stop\n",
      "\n",
      "Message content: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Get full response object\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESPONSE OBJECT STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel used: {response.model}\")\n",
    "print(f\"ID: {response.id}\")\n",
    "print(f\"Created: {response.created}\")\n",
    "print(f\"\\nToken usage:\")\n",
    "print(f\"  - Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"  - Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"  - Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"\\nFinish reason: {response.choices[0].finish_reason}\")\n",
    "print(f\"\\nMessage content: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Tokens and Tokenization\n",
    "\n",
    "Tokens are the fundamental units that LLMs process. Understanding tokenization is crucial for:\n",
    "- Cost estimation (pricing is per token)\n",
    "- Context window management\n",
    "- Prompt engineering\n",
    "\n",
    "### What is a Token?\n",
    "- A token is a piece of text (not exactly a word)\n",
    "- Common words = 1 token (e.g., \"cat\")\n",
    "- Uncommon words = multiple tokens (e.g., \"unconventional\" ‚âà 3 tokens)\n",
    "- 1 token ‚âà 4 characters in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hello, world!'\n",
      "Length: 13 characters\n",
      "Tokens: 4\n",
      "Token IDs: [9906, 11, 1917, 0]...\n",
      "Chars per token: 3.25\n",
      "------------------------------------------------------------\n",
      "Text: 'The quick brown fox jumps over the lazy dog.'\n",
      "Length: 44 characters\n",
      "Tokens: 10\n",
      "Token IDs: [791, 4062, 14198, 39935, 35308, 927, 279, 16053, 5679, 13]...\n",
      "Chars per token: 4.40\n",
      "------------------------------------------------------------\n",
      "Text: 'Supercalifragilisticexpialidocious'\n",
      "Length: 34 characters\n",
      "Tokens: 11\n",
      "Token IDs: [10254, 3035, 278, 333, 4193, 321, 4633, 4683, 532, 307, 78287]...\n",
      "Chars per token: 3.09\n",
      "------------------------------------------------------------\n",
      "Text: 'AI and ML are transforming technology.'\n",
      "Length: 38 characters\n",
      "Tokens: 7\n",
      "Token IDs: [15836, 323, 20187, 527, 46890, 5557, 13]...\n",
      "Chars per token: 5.43\n",
      "------------------------------------------------------------\n",
      "Text: '‰∫∫Â∑•Êô∫ËÉΩÊ≠£Âú®ÊîπÂèò‰∏ñÁïå'\n",
      "Length: 10 characters\n",
      "Tokens: 11\n",
      "Token IDs: [17792, 49792, 45114, 118, 27327, 97655, 23226, 75140, 3574, 244, 98220]...\n",
      "Chars per token: 0.91\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def count_and_display_tokens(text: str, model: str = \"gpt-4\"):\n",
    "    \"\"\"Count tokens and show token breakdown.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Length: {len(text)} characters\")\n",
    "    print(f\"Tokens: {len(tokens)}\")\n",
    "    print(f\"Token IDs: {tokens[:20]}...\")  # Show first 20\n",
    "    print(f\"Chars per token: {len(text) / len(tokens):.2f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Test with different texts\n",
    "texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Supercalifragilisticexpialidocious\",\n",
    "    \"AI and ML are transforming technology.\",\n",
    "    \"‰∫∫Â∑•Êô∫ËÉΩÊ≠£Âú®ÊîπÂèò‰∏ñÁïå\"  # Chinese text\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    count_and_display_tokens(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Counting with Our Custom Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 226 characters\n",
      "Estimated tokens: 48\n",
      "\n",
      "Text preview:\n",
      "Large Language Models (LLMs) are advanced AI systems trained on vast amounts\n",
      "of text data. They can understand and generate human-like text, making them useful\n",
      "for tasks like translation, summarization, and question-answering.\n"
     ]
    }
   ],
   "source": [
    "# Using our custom utility\n",
    "text = \"\"\"Large Language Models (LLMs) are advanced AI systems trained on vast amounts\n",
    "of text data. They can understand and generate human-like text, making them useful\n",
    "for tasks like translation, summarization, and question-answering.\"\"\"\n",
    "\n",
    "token_count = count_tokens(text, model=\"gpt-4\")\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Estimated tokens: {token_count}\")\n",
    "print(f\"\\nText preview:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temperature and Sampling Parameters\n",
    "\n",
    "Temperature controls the randomness of the model's output:\n",
    "- **Temperature = 0**: Deterministic, always picks most likely token (good for factual tasks)\n",
    "- **Temperature = 0.3-0.7**: Balanced creativity and consistency\n",
    "- **Temperature = 1.0+**: More random and creative (good for creative writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of France?\n",
      "================================================================================\n",
      "\n",
      "üå°Ô∏è  Temperature: 0.0\n",
      "Response: The capital of France is Paris.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üå°Ô∏è  Temperature: 0.7\n",
      "Response: The capital of France is Paris.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üå°Ô∏è  Temperature: 1.5\n",
      "Response: The capital of France is Paris.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def compare_temperatures(prompt: str, temperatures: list):\n",
    "    \"\"\"Compare outputs at different temperatures.\"\"\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for temp in temperatures:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            max_tokens=100\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüå°Ô∏è  Temperature: {temp}\")\n",
    "        print(f\"Response: {response.choices[0].message.content}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Test with a factual question\n",
    "compare_temperatures(\n",
    "    \"What is the capital of France?\",\n",
    "    temperatures=[0.0, 0.7, 1.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a creative tagline for an AI startup.\n",
      "================================================================================\n",
      "\n",
      "üå°Ô∏è  Temperature: 0.0\n",
      "Response: \"Empowering the future with intelligent technology.\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üå°Ô∏è  Temperature: 0.7\n",
      "Response: \"Empowering the future with intelligent technology.\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üå°Ô∏è  Temperature: 1.5\n",
      "Response: \"Evolving intelligence, extending possibilities.\"\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with a creative task\n",
    "compare_temperatures(\n",
    "    \"Write a creative tagline for an AI startup.\",\n",
    "    temperatures=[0.0, 0.7, 1.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short response (max_tokens=50):\n",
      "Quantum computing is a type of computing that uses principles of quantum mechanics to perform operations. Traditional computers use bits, which can be either a 0 or a 1, to perform calculations. Quantum computers, on the other hand, use quantum bits\n",
      "\n",
      "Tokens used: 50\n",
      "\n",
      "============================================================\n",
      "\n",
      "Creative output (top_p=0.9, temperature=0.8):\n",
      "Artificial minds\n",
      "Learning and evolving fast\n",
      "Future now in hand\n"
     ]
    }
   ],
   "source": [
    "# max_tokens: Limit response length\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing.\"}],\n",
    "    max_tokens=50  # Short response\n",
    ")\n",
    "print(\"Short response (max_tokens=50):\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nTokens used: {response.usage.completion_tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# top_p: Nucleus sampling (alternative to temperature)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku about AI.\"}],\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(\"Creative output (top_p=0.9, temperature=0.8):\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. System Messages and Conversation Context\n",
    "\n",
    "System messages set the behavior and context for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical Expert Response:\n",
      "Optimizing a loop that processes 1 million items in Python is important to ensure efficient execution and minimize processing time. Here are some tips to optimize your loop:\n",
      "\n",
      "1. **Use List Comprehension**: List comprehension can be more efficient than traditional loops for simple operations. If you can express your processing logic using list comprehension, it can be faster.\n",
      "\n",
      "2. **Minimize Function Calls**: If there are function calls inside the loop that are not necessary for each iteration (e.g., calling the same function with the same arguments multiple times), consider moving them outside the loop.\n",
      "\n",
      "3. **Avoid Unnecessary Calculations**: Make sure your loop only performs calculations that are necessary. Avoid unnecessary checks or calculations inside the loop that can be done before the loop starts.\n",
      "\n",
      "4. **Use Built-in Functions**: Utilize built-in functions like `map()`, `filter()`, and `reduce()` where appropriate. These functions are optimized for performance and can be more efficient than manually iterating over a list.\n",
      "\n",
      "5. **Use Generators**: If possible, consider using generators instead of lists to avoid storing all 1 million items in memory at once. Generators are memory-efficient and can be iterated over lazily.\n",
      "\n",
      "6. **Parallelize Processing**: If the processing of each item is independent of others, you can consider parallelizing the loop using Python's multiprocessing or threading libraries to take advantage of multiple CPU cores.\n",
      "\n",
      "7. **Profile Your Code**: Use Python profiling tools such as cProfile to identify bottlenecks in your code. This can help you pinpoint areas that need optimization.\n",
      "\n",
      "8. **Optimize Data Structures**: If you are frequently accessing elements in a list, consider using a more optimized data structure like a set or a dictionary for faster lookups.\n",
      "\n",
      "9. **Optimize Algorithms**: Ensure that the algorithm used in your loop is efficient for the problem you are trying to solve. Sometimes, optimizing the algorithm itself can result in significant performance improvements.\n",
      "\n",
      "10. **Consider Using Numba or Cython**: If performance is critical and you have a lot of numerical computations, consider using Numba or Cython to compile your code to machine code and achieve significant speedups.\n",
      "\n",
      "By applying these optimization strategies, you can improve the performance of your loop that processes 1 million items in Python. Remember, optimization should always be guided by profiling and measuring actual performance gains.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Simple Explainer Response:\n",
      "Imagine you have a big pile of 1 million toys to sort through. Instead of going through each toy one by one, it would be faster if you grouped the toys by color or type first. \n",
      "\n",
      "In computer programming, optimizing a loop is like finding a quicker way to sort through the toys. One way to do this is by using techniques like breaking down the task into smaller parts, making use of efficient algorithms, or using tools that can help speed up the process.\n",
      "\n",
      "So, when you optimize a loop that processes 1 million items, you're basically finding ways to make the process faster and more efficient, just like sorting through those toys in a smarter way.\n"
     ]
    }
   ],
   "source": [
    "def chat_with_system_message(system_msg: str, user_msg: str):\n",
    "    \"\"\"Make a call with a system message.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example 1: Technical expert\n",
    "response1 = chat_with_system_message(\n",
    "    system_msg=\"You are a senior software engineer with expertise in Python.\",\n",
    "    user_msg=\"How do I optimize a loop that processes 1 million items?\"\n",
    ")\n",
    "print(\"Technical Expert Response:\")\n",
    "print(response1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Example 2: Simple explainer\n",
    "response2 = chat_with_system_message(\n",
    "    system_msg=\"You explain complex topics to 10-year-olds using simple language and analogies.\",\n",
    "    user_msg=\"How do I optimize a loop that processes 1 million items?\"\n",
    ")\n",
    "print(\"Simple Explainer Response:\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's the capital of France?\n",
      "Assistant: The capital of France is Paris.\n",
      "\n",
      "User: What's its population?\n",
      "Assistant: As of 2021, the population of Paris, France is estimated to be around 2.2 million people.\n"
     ]
    }
   ],
   "source": [
    "# Maintaining conversation context\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "]\n",
    "\n",
    "# First exchange\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=conversation\n",
    ")\n",
    "assistant_reply = response.choices[0].message.content\n",
    "conversation.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "print(\"User: What's the capital of France?\")\n",
    "print(f\"Assistant: {assistant_reply}\\n\")\n",
    "\n",
    "# Follow-up question (references previous context)\n",
    "conversation.append({\"role\": \"user\", \"content\": \"What's its population?\"})\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=conversation\n",
    ")\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "print(\"User: What's its population?\")\n",
    "print(f\"Assistant: {assistant_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "\n",
    "Streaming allows to get responses token-by-token as they're generated, improving perceived latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response: Neural networks are a type of machine learning model inspired by the structure of the human brain. They consist of layers of interconnected nodes, or artificial neurons, that process and learn from data. By adjusting the weights and connections between neurons, neural networks can recognize patterns and make predictions in a wide range of applications, from image recognition to natural language processing.\n"
     ]
    }
   ],
   "source": [
    "def stream_completion(prompt: str):\n",
    "    \"\"\"Stream a completion response.\"\"\"\n",
    "    print(\"Streaming response: \", end=\"\", flush=True)\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "\n",
    "    print()  # New line\n",
    "    return full_response\n",
    "\n",
    "# Test streaming\n",
    "response = stream_completion(\n",
    "    \"Explain the concept of neural networks in 3 sentences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Estimation\n",
    "\n",
    "Understanding and tracking costs is crucial when working with LLM APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated input tokens: 12\n",
      "Estimated cost: $0.000231\n",
      "------------------------------------------------------------\n",
      "\n",
      "Actual tokens:\n",
      "  Input: 19\n",
      "  Output: 175\n",
      "  Total: 194\n",
      "Actual cost: $0.000272\n",
      "\n",
      "Response:\n",
      "Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning the input data is paired with the correct output. The algorithm learns to map the input to the output based on the labeled examples provided during training. Common types of supervised learning algorithms include classification and regression.\n",
      "\n",
      "On the other hand, unsupervised learning is a type of machine learning where the algorithm is trained on an unlabeled dataset, meaning there is no corresponding output given for the input data. The algorithm must learn the inherent patterns and structures in the data without any explicit guidance. Common types of unsupervised learning algorithms include clustering and dimensionality reduction.\n",
      "\n",
      "In summary, the main difference between supervised and unsupervised learning is the presence or absence of labeled data during the training process. Supervised learning requires labeled data to train the algorithm, while unsupervised learning does not.\n"
     ]
    }
   ],
   "source": [
    "def estimate_and_call(prompt: str, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"Estimate cost before making the call.\"\"\"\n",
    "    # Estimate input tokens\n",
    "    input_tokens = count_tokens(prompt, model)\n",
    "\n",
    "    # Estimate cost (assuming ~150 output tokens)\n",
    "    estimated_cost = CostEstimator.estimate_cost(\n",
    "        model=model,\n",
    "        input_tokens=input_tokens,\n",
    "        output_tokens=150\n",
    "    )\n",
    "\n",
    "    print(f\"Estimated input tokens: {input_tokens}\")\n",
    "    print(f\"Estimated cost: ${estimated_cost:.6f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Make the actual call\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    # Calculate actual cost\n",
    "    actual_cost = CostEstimator.estimate_cost(\n",
    "        model=model,\n",
    "        input_tokens=response.usage.prompt_tokens,\n",
    "        output_tokens=response.usage.completion_tokens\n",
    "    )\n",
    "\n",
    "    print(f\"\\nActual tokens:\")\n",
    "    print(f\"  Input: {response.usage.prompt_tokens}\")\n",
    "    print(f\"  Output: {response.usage.completion_tokens}\")\n",
    "    print(f\"  Total: {response.usage.total_tokens}\")\n",
    "    print(f\"Actual cost: ${actual_cost:.6f}\")\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test it\n",
    "response = estimate_and_call(\n",
    "    \"Explain the difference between supervised and unsupervised learning.\"\n",
    ")\n",
    "print(f\"\\nResponse:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Costs Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Comparison for Same Task:\n",
      "============================================================\n",
      "gpt-3.5-turbo       : $0.000275\n",
      "gpt-4               : $0.012000\n",
      "gpt-4-turbo         : $0.005500\n"
     ]
    }
   ],
   "source": [
    "# Compare costs for different models\n",
    "prompt = \"Write a 100-word summary of machine learning.\"\n",
    "models = [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo\"]\n",
    "\n",
    "input_tokens = 100  # Approximate\n",
    "output_tokens = 150  # Approximate\n",
    "\n",
    "print(\"Cost Comparison for Same Task:\")\n",
    "print(\"=\" * 60)\n",
    "for model in models:\n",
    "    cost = CostEstimator.estimate_cost(model, input_tokens, output_tokens)\n",
    "    print(f\"{model:20s}: ${cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing Our Custom Utils\n",
    "\n",
    "Let's test the utility functions we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slow_completion took 0.81 seconds\n",
      "\n",
      "Result: Python was named after the British comedy show \"Monty Python's Flying Circus\" and not the snake as many people assume. Guido van Rossum, the creator of Python, is a fan of the show and named the programming language in honor of it.\n"
     ]
    }
   ],
   "source": [
    "# Test timer decorator\n",
    "@timer\n",
    "def slow_completion(prompt: str):\n",
    "    \"\"\"A completion that we'll time.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "result = slow_completion(\"Tell me a fun fact about Python.\")\n",
    "print(f\"\\nResult: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from environment:\n",
      "Model: gpt-3.5-turbo\n",
      "Temperature: 0.0\n",
      "Max tokens: 2000\n",
      "\n",
      "Response: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Test Config utility\n",
    "config = Config.from_env()\n",
    "\n",
    "print(\"Configuration loaded from environment:\")\n",
    "print(f\"Model: {config.get('model')}\")\n",
    "print(f\"Temperature: {config.get('temperature')}\")\n",
    "print(f\"Max tokens: {config.get('max_tokens')}\")\n",
    "\n",
    "# Use config in a call\n",
    "response = client.chat.completions.create(\n",
    "    model=config.get('model'),\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    temperature=config.get('temperature'),\n",
    "    max_tokens=config.get('max_tokens')\n",
    ")\n",
    "print(f\"\\nResponse: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices Summary\n",
    "\n",
    "### When to Use Different Settings:\n",
    "\n",
    "| Task Type | Temperature | Model | Why |\n",
    "|-----------|-------------|-------|-----|\n",
    "| Factual Q&A | 0.0 - 0.3 | GPT-3.5-Turbo | Consistent, cheap |\n",
    "| Data extraction | 0.0 | GPT-3.5-Turbo | Deterministic output |\n",
    "| Creative writing | 0.7 - 1.0 | GPT-4 | More creative, high quality |\n",
    "| Code generation | 0.0 - 0.3 | GPT-4 | Reliable, accurate |\n",
    "| Summarization | 0.3 - 0.5 | GPT-3.5-Turbo | Balanced |\n",
    "| Brainstorming | 0.8 - 1.2 | GPT-4 | Diverse ideas |\n",
    "\n",
    "### Cost Optimization Tips:\n",
    "1. Use GPT-3.5-Turbo for simple tasks\n",
    "2. Keep prompts concise\n",
    "3. Use `max_tokens` to limit output\n",
    "4. Cache common responses\n",
    "5. Batch similar requests\n",
    "\n",
    "### Common Pitfalls:\n",
    "1. Not counting tokens properly\n",
    "2. Using GPT-4 when GPT-3.5 suffices\n",
    "3. Forgetting to handle rate limits\n",
    "4. Not streaming for long responses\n",
    "5. Ignoring token context limits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-portfolio-fuSmwnhh-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
